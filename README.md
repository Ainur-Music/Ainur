# Ainur: Conditional Multimodal Deep Music Generation

[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![Python](https://img.shields.io/badge/python-3.8%20%7C%203.9%20%7C%204.0%20%7C%204.1-blue)](https://www.python.org/downloads/release)
[![PyTorch](https://img.shields.io/badge/pytorch-1.13.1-blue)](https://pytorch.org/get-started/locally/)
[![PyTorch Lightning](https://img.shields.io/badge/pytorch_lightning-2.0.0-blue)](https://pytorch-lightning.readthedocs.io/en/stable/)

![ainur architecture (training & inference)](assets/ainur.png)
Ainur is an innovative deep learning model for conditional multimodal music generation. It is designed to generate high-quality stereo music samples at 48 kHz conditioned on a variety of inputs, such as lyrics, text descriptors, and other audio. Ainur's hierarchical diffusion architecture, combined with CLASP embeddings, allows it to produce coherent and expressive music compositions across a wide range of genres and styles.

## Features

- **Conditional Generation:** Ainur enables the generation of music conditioned on lyrics, text descriptors, or other audio, offering a flexible and creative approach to music composition.

- **High-Quality Output:** The model is capable of producing 22-second stereo music samples at 48 kHz, ensuring high fidelity and realism.

- **Multimodal Learning:** Ainur employs CLASP embeddings, which are multimodal representations of lyrics and audio, to facilitate the alignment of textual lyrics with corresponding audio fragments.

- **Objective Evaluation:** We provide comprehensive evaluation metrics, including Frechet Audio Distance (FAD) and CLASP Cycle Consistency (C3), to assess the quality and coherence of generated music.

## Requirements

To run Ainur, ensure you have the following dependencies installed:

- Python 3.8+
- PyTorch 1.13.1
- PyTorch Lightning 2.0.0

You can install the required Python packages by running:

```bash
pip install -r requirements.txt
```

## Usage
1. Clone this repository:
```bash
git clone https://github.com/ainur-music/ainur.git
cd ainur
```
2. Install the dependencies (as mentioned above).
3. Run Ainur with your desired input. Check out the example notebooks in the `examples` folder for guidance on using Ainur for music generation. (*coming soon*)

## Evaluation
To assess the quality and performance of Ainur, refer to the [Ainur Evaluation Guide](https://github.com/Ainur-Music/Ainur/blob/main/eval.md) for detailed instructions and metrics.

## Publications
Read the research thesis on Ainur [here](https://indigo.uic.edu/articles/thesis/Ainur_Enhancing_Vocal_Quality_through_Lyrics-Audio_Embeddings_in_Multimodal_Deep_Music_Generation/24241876).
```
@masterthesis{ainur-thesis,
  author = {G. Concialdi, A. Koudounas, E. Pastor, E. Baralis, B. Di Eugenio},
  title = {Ainur: Enhancing Vocal Quality through Lyrics-Audio Embeddings in Multimodal Deep Music Generation},
  school = {University of Illinois at Chicago},
  year = {2023},
  month = {August},
  doi = {10.25417/uic.24241876.v1}
}
```
## Audio Samples
Explore and listen to music generated by Ainur on our [webpage](http://about:blank).

## Checkpoints
You can download pre-trained Ainur and CLASP checkpoints from the following links:
- Ainur Checkpoint
- CLASP Checkpoint

## License
This project is licensed under the MIT License - see the [LICENSE](https://github.com/Ainur-Music/Ainur/blob/main/LICENSE) file for details.

----
Â© 2023 Giuseppe Concialdi
